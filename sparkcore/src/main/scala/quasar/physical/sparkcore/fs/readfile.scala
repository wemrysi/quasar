/*
 * Copyright 2014â€“2017 SlamData Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package quasar.physical.sparkcore.fs

import slamdata.Predef._
import quasar.Data
import quasar.contrib.pathy._
import quasar.effect._
import quasar.fp.free._
import quasar.fp.numeric.{Natural, Positive}
import quasar.fs._, FileSystemError._

import org.apache.spark.SparkContext
import scalaz._, Scalaz._
import scalaz.concurrent.Task

object readfile {

  type Offset = Natural
  type Limit = Option[Positive]

  import ReadFile.ReadHandle

  def interpret[S[_]](implicit
    s0: KeyValueStore[ReadHandle, SparkCursor, ?] :<: S,
    s1: Read[SparkContext, ?] :<: S,
    s2: MonotonicSeq :<: S,
    s3: Task :<: S,
    details: SparkConnectorDetails.Ops[S]
  ): ReadFile ~> Free[S, ?] =
    new (ReadFile ~> Free[S, ?]) {
      def apply[A](rf: ReadFile[A]) = rf match {
        case ReadFile.Open(f, offset, limit) => open[S](f, offset, limit)
        case ReadFile.Read(h) =>  read[S](h)
        case ReadFile.Close(h) => close[S](h)
      }
  }

  def open[S[_]](f: AFile, offset: Offset, limit: Limit)(implicit
    kvs: KeyValueStore.Ops[ReadHandle, SparkCursor, S],
    s1: Read[SparkContext, ?] :<: S,
    gen: MonotonicSeq.Ops[S],
    details: SparkConnectorDetails.Ops[S]
  ): Free[S, FileSystemError \/ ReadHandle] = {

    def freshHandle: Free[S, ReadHandle] =
      gen.next map (ReadHandle(f, _))

    def _open: Free[S, ReadHandle] = for {
      rdd <- details.rddFrom(f)
      limitedRdd = rdd.zipWithIndex().filter {
        case (value, index) =>
          limit.fold(
            index >= offset.value
          ) (
            limit => index >= offset.value && index < limit.value + offset.value
          )
      }
      cur = SparkCursor(limitedRdd.some, 0)
      h <- freshHandle
      _ <- kvs.put(h, cur)
    } yield h

    def _empty: Free[S, ReadHandle] = for {
      h <- freshHandle
      cur = SparkCursor(None, 0)
      _ <- kvs.put(h, cur)
    } yield h

    details.fileExists(f).ifM(
      _open map (_.right[FileSystemError]),
     _empty map (_.right[FileSystemError])
    )
  }

  def read[S[_]](h: ReadHandle)(implicit
    kvs: KeyValueStore.Ops[ReadHandle, SparkCursor, S],
    s1: Task :<: S,
    details: SparkConnectorDetails.Ops[S]
  ): Free[S, FileSystemError \/ Vector[Data]] = 
    details.readChunkSize >>= (step => kvs.get(h).toRight(unknownReadHandle(h)).flatMap {
      case SparkCursor(None, _) =>
        Vector.empty[Data].pure[EitherT[Free[S, ?], FileSystemError, ?]]
      case SparkCursor(Some(rdd), p) =>

        val collect = lift(Task.delay {
          rdd
            .filter(d => d._2 >= p && d._2 < (p + step))
            .map(_._1).collect.toVector
        }).into[S].liftM[FileSystemErrT]

        for {
          collected <- collect
          cur = if(collected.isEmpty) SparkCursor(None, 0) else SparkCursor(some(rdd), p + step)
          _ <- kvs.put(h, cur).liftM[FileSystemErrT]
        } yield collected
        
    }.run)

  def close[S[_]](h: ReadHandle)(implicit
    kvs: KeyValueStore.Ops[ReadHandle, SparkCursor, S]
  ): Free[S, Unit] = kvs.delete(h)
}
